<!DOCTYPE html>
<html class="writer-html4" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Welcome to Py2vision’s documentation! &mdash; py2vision 1.0 documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Compute" href="module_compute.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> py2vision
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="module_compute.html">Compute</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_datasets_loader.html">Datasets loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_image_process.html">Image process</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_input_output.html">Inputs and Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_models.html">Tensorflow models</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_recognition.html">Object detection (Recognition)</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_stereo.html">Stereo Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_utils.html">Utilities</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">py2vision</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>Welcome to Py2vision’s documentation!</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="welcome-to-py2vision-s-documentation">
<h1>Welcome to Py2vision’s documentation!<a class="headerlink" href="#welcome-to-py2vision-s-documentation" title="Permalink to this headline">¶</a></h1>
<p>Here you can see every detail about py2vision package which is a tool to do object positioning with stereo vision and object detection, this package can be used in Robotics and Computer Vision.</p>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Open a console and write this:
<strong class="command">pip install py2vision</strong></p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">If you already have the opencv package installed, you have to uninstall it with <strong class="command">pip uninstall opencv-python</strong> and install opencv-contrib-python version 4.6.0.66.</p>
</div>
</div>
<div class="section" id="what-could-you-archieve-with-py2vision">
<h2>What could you archieve with py2vision?<a class="headerlink" href="#what-could-you-archieve-with-py2vision" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li>It will allow you to obtain disparity maps using SGBM algorithm.</li>
<li>Get distances in the 3D space.</li>
<li>Measure objects dimensions and more.</li>
<li>Train your own object detection model and infer with it.</li>
<li>Combine detection and stereo vision to get homogeneous coordinates of objects im an scene.</li>
</ol>
<div class="figure align-center" id="id22">
<a class="reference internal image-reference" href="https://github.com/corvus96/PyTwoVision/blob/master/source/doc_static/disparity_map_example.jpg?raw=true"><img alt="https://github.com/corvus96/PyTwoVision/blob/master/source/doc_static/disparity_map_example.jpg?raw=true" src="https://github.com/corvus96/PyTwoVision/blob/master/source/doc_static/disparity_map_example.jpg?raw=true" style="width: 250px; height: 250px;" /></a>
<p class="caption"><span class="caption-text">Example of a disparity map</span></p>
</div>
<div class="figure align-center" id="id23">
<a class="reference internal image-reference" href="https://github.com/corvus96/PyTwoVision/blob/master/source/doc_static/position_example.jpg?raw=true"><img alt="https://github.com/corvus96/PyTwoVision/blob/master/source/doc_static/position_example.jpg?raw=true" src="https://github.com/corvus96/PyTwoVision/blob/master/source/doc_static/position_example.jpg?raw=true" style="width: 300px; height: 300px;" /></a>
<p class="caption"><span class="caption-text">Example of positioning of objects</span></p>
</div>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="module_compute.html">Compute</a><ul>
<li class="toctree-l2"><a class="reference internal" href="module_compute.html#calculate-the-reprojection-error">Calculate the reprojection error</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_compute.html#yolo-v3-computations">Yolo V3 Computations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module_datasets_loader.html">Datasets loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_image_process.html">Image process</a><ul>
<li class="toctree-l2"><a class="reference internal" href="module_image_process.html#the-epicenter-of-this-module">The epicenter of this module</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_image_process.html#image-transformations">Image transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_image_process.html#how-to-use">How to use?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module_input_output.html">Inputs and Outputs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="module_input_output.html#module-py2vision.input_output.camera">Inputs like</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_input_output.html#module-py2vision.input_output.vision_system">Outputs like</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module_models.html">Tensorflow models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="module_models.html#blocks">Blocks</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_models.html#layers">Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_models.html#models-manager">Models manager</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module_recognition.html">Object detection (Recognition)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="module_recognition.html#networks-selector">Networks selector</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_recognition.html#yolov3-implementation">YOLOV3 implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_recognition.html#inference-modes">Inference modes</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_recognition.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module_stereo.html">Stereo Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="module_stereo.html#implement-sgbm-algorithm">Implement SGBM Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_stereo.html#standard-stereo-builder">Standard Stereo Builder</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_stereo.html#stereo-controller">Stereo Controller</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_stereo.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module_utils.html">Utilities</a><ul>
<li class="toctree-l2"><a class="reference internal" href="module_utils.html#annotations-parser">Annotations Parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_utils.html#annotations-helper">Annotations Helper</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_utils.html#draw-functions">Draw functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="module_utils.html#label-utils">Label utils</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="dependencies">
<h2>Dependencies<a class="headerlink" href="#dependencies" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>numpy == 1.21.5</li>
<li>tensorflow == 2.8.0</li>
<li>opencv-contrib-python==4.6.0.66</li>
<li>wget == 3.2</li>
<li>pandas</li>
<li>pyyaml</li>
<li>h5py</li>
</ul>
<div class="section" id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><ol class="first last upperalpha simple" start="3">
<li>Wheatstone, «Contributions to the Physiology of Vision.», Proceedings</li>
</ol>
</td></tr>
</tbody>
</table>
<p>of the Royal Society of London, vol. 4, 0 1837.</p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><ol class="first last upperalpha simple" start="13">
<li><ol class="first upperalpha" start="13">
<li>Martín, Técnicas de visión estereoscópica para determinar la estruc-</li>
</ol>
</li>
</ol>
</td></tr>
</tbody>
</table>
<p>tura tridimensional de la escena Proyecto, Madrid, 2010.</p>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><ol class="first last upperalpha simple" start="10">
<li>Dembys, Y. Gao, A. Shafiekhani y G. Desouza, Object Detection and Pose</li>
</ol>
</td></tr>
</tbody>
</table>
<p>Estimation Using CNN in Embedded Hardware for Assistive Technology,
oct. de 2019.</p>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><ol class="first last upperalpha simple" start="8">
<li>Königshof, N. O. Salscheider y C. Stiller, «Realtime 3D Object Detection</li>
</ol>
</td></tr>
</tbody>
</table>
<p>for Automated Driving Using Stereo Vision and Semantic Information», en
2019 IEEE Intelligent Transportation Systems Conference (ITSC), 2019,
págs. 1405-1410. doi: 10.1109/ITSC.2019.8917330.</p>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td><ol class="first last upperalpha simple" start="19">
<li><ol class="first upperalpha" start="20">
<li>Barnard y M. A. Fischler, «Computational Stereo», ACM Computing</li>
</ol>
</li>
</ol>
</td></tr>
</tbody>
</table>
<p>Surveys (CSUR), vol. 14, 4 1982, issn: 15577341. doi: 10.1145/356893.
356896.</p>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td><ol class="first last upperalpha simple" start="6">
<li>Torres, J. Pomares, P. Gil, S. T. Puente y R. Aracil, Robots y Sistemas</li>
</ol>
</td></tr>
</tbody>
</table>
<p>Sensoriales. Madrid: Pearson Education, 2002, págs. 69-94.</p>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[7]</td><td><ol class="first last upperalpha simple" start="26">
<li>Zhang, «A flexible new technique for camera calibration», IEEE Transac-</li>
</ol>
</td></tr>
</tbody>
</table>
<p>tions on Pattern Analysis and Machine Intelligence, vol. 22, 11 2000, issn:
01628828. doi: 10.1109/34.888718.</p>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td><ol class="first last upperalpha simple" start="18">
<li><ol class="first upperroman">
<li>Hartley, «Theory and practice of projective rectification», International</li>
</ol>
</li>
</ol>
</td></tr>
</tbody>
</table>
<p>Journal of Computer Vision, vol. 35, 2 1999, issn: 09205691. doi: 10.1023/
A:1008115206617.</p>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[9]</td><td><ol class="first last upperalpha simple" start="4">
<li>Scharstein y R. Szeliski, «A taxonomy and evaluation of dense two-frame</li>
</ol>
</td></tr>
</tbody>
</table>
<p>stereo correspondence algorithms», International Journal of Computer Vi-
sion, vol. 47, 1-3 2002, issn: 09205691. doi: 10.1023/A:1014573219977.</p>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[10]</td><td><ol class="first last upperalpha simple" start="18">
<li>Szeliski, Computer Vision : Algorithms and Applications 2nd edition.</li>
</ol>
</td></tr>
</tbody>
</table>
<ol class="arabic simple" start="2020">
<li></li>
</ol>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[11]</td><td><ol class="first last upperalpha simple" start="19">
<li>Dai y W. Huang, A-TVSNet: Aggregated Two-View Stereo Network for</li>
</ol>
</td></tr>
</tbody>
</table>
<p>Multi-View Stereo Depth Estimation, 2020. arXiv: 2003.00711 [cs.CV].</p>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[12]</td><td>CS231n Convolutional Neural Networks for Visual Recognition, Accesado el</td></tr>
</tbody>
</table>
<p>12, de febrero de 2021. dirección: <a class="reference external" href="https://cs231n.github.io/convolutional">https://cs231n.github.io/convolutional</a>-
networks/.</p>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[13]</td><td><ol class="first last upperalpha simple" start="11">
<li>Kar, Mastering Computer Vision with TensorFlow 2.x. 2020.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id14" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[14]</td><td><ol class="first last upperalpha simple" start="11">
<li>He, X. Zhang, S. Ren y J. Sun, Identity Mappings in Deep Residual</li>
</ol>
</td></tr>
</tbody>
</table>
<p>Networks, 2016. doi: 10 . 48550 / ARXIV . 1603 . 05027. dirección: https :
//arxiv.org/abs/1603.05027.</p>
<table class="docutils footnote" frame="void" id="id15" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[15]</td><td><ol class="first last upperalpha simple" start="18">
<li>Atienza, Advanced Deep Learning with Keras: Apply deep learning tech-</li>
</ol>
</td></tr>
</tbody>
</table>
<p>niques, autoencoders, GANs, variational autoencoders, deep reinforcement
learning, policy gradients, and more. 2018.</p>
<table class="docutils footnote" frame="void" id="id16" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[16]</td><td><ol class="first last upperalpha simple" start="10">
<li>Long, E. Shelhamer y T. Darrell, Fully Convolutional Networks for Se-</li>
</ol>
</td></tr>
</tbody>
</table>
<p>mantic Segmentation, 2014. doi: 10.48550/ARXIV.1411.4038. dirección:
<a class="reference external" href="https://arxiv.org/abs/1411.4038">https://arxiv.org/abs/1411.4038</a>.</p>
<table class="docutils footnote" frame="void" id="id17" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[17]</td><td>——, StereoPi V2 quick start guide, 2014. dirección: <a class="reference external" href="https://wiki.stereopi">https://wiki.stereopi</a>.</td></tr>
</tbody>
</table>
<p>com/index.php?title=StereoPi_v2_Quick_Start_Guide.</p>
<table class="docutils footnote" frame="void" id="id18" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[18]</td><td><ol class="first last upperalpha simple" start="7">
<li><ol class="first upperalpha" start="2">
<li>Adrian Kaehler, Learning OpenCV 3: computer vision in C++ with</li>
</ol>
</li>
</ol>
</td></tr>
</tbody>
</table>
<p>the OpenCV library. California, USA: O’Reilly Media, Inc., 2016.</p>
<table class="docutils footnote" frame="void" id="id19" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[19]</td><td>T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan y S. Belongie, Feature</td></tr>
</tbody>
</table>
<p>Pyramid Networks for Object Detection, 2016. doi: 10.48550/ARXIV.1612.
03144. dirección: <a class="reference external" href="https://arxiv.org/abs/1612.03144">https://arxiv.org/abs/1612.03144</a>.</p>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[20]</td><td><ol class="first last upperalpha simple" start="10">
<li>Redmon y A. Farhadi, «YOLOv3: An Incremental Improvement», arXiv,</li>
</ol>
</td></tr>
</tbody>
</table>
<ol class="arabic simple" start="2018">
<li></li>
</ol>
<table class="docutils footnote" frame="void" id="id21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[21]</td><td><ol class="first last upperalpha simple" start="13">
<li>Everingham, L. Van Gool, C. K. I. Williams, J. Winn y A. Zisserman,</li>
</ol>
</td></tr>
</tbody>
</table>
<p>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results,
<a class="reference external" href="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html">http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html</a>.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="module_compute.html" class="btn btn-neutral float-right" title="Compute" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Guillermo Raven.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>